{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a3d493-83fb-4965-bdcb-f1054425e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geojson\n",
      "  Downloading geojson-3.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading geojson-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: geojson\n",
      "Successfully installed geojson-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Downloading shapely-2.1.2-cp311-cp311-win_amd64.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\422mi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shapely) (2.2.6)\n",
      "Downloading shapely-2.1.2-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 47.1 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyShp\n",
      "  Downloading pyshp-3.0.2.post1-py3-none-any.whl.metadata (64 kB)\n",
      "Downloading pyshp-3.0.2.post1-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: PyShp\n",
      "Successfully installed PyShp-3.0.2.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\422mi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dbfread\n",
      "  Downloading dbfread-2.0.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
      "Installing collected packages: dbfread\n",
      "Successfully installed dbfread-2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install geojson\n",
    "!pip install shapely\n",
    "!pip install PyShp\n",
    "!pip install networkx\n",
    "!pip install dbfread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a7f250-1f24-4f02-832b-1df2dceaa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "import ast\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Polygon,shape,MultiPolygon\n",
    "import shapely.ops\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959227a9-93f8-47fa-87ef-02037facafb9",
   "metadata": {},
   "source": [
    "# We will work on New Hampshire\n",
    "The data is in Canvas, you should upload it to your Google Drive first (if using Colab), or local filesystem (if using Jupyter).\n",
    "The NJ data is formatted the same way, just replace 'nh' with 'nj'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69143d08-629d-436a-8687-7a5d3bcb5aa3",
   "metadata": {},
   "source": [
    "### This is the current assignment of precinct to congressional districts (2 of them for NH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c2b03-9b2f-46c5-b619-0709ca555c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Map_Data/precinct-assignments-congress-nh.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nh_current_assignment \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMap_Data/precinct-assignments-congress-nh.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m nh_current_assignment\n",
      "File \u001b[1;32mc:\\Users\\422mi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\422mi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\422mi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\422mi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\422mi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Map_Data/precinct-assignments-congress-nh.csv'"
     ]
    }
   ],
   "source": [
    "nh_current_assignment = pd.read_csv('Map_Data/precinct-assignments-congress-nj.csv')\n",
    "nh_current_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bce06-2f20-4f82-959e-a95f91e25ba5",
   "metadata": {},
   "source": [
    "### This is the current demographic and voter data\n",
    "The data has a lot of attributes that lists voters of different demographics and parties in different elections. You can look at the data Dictionary on Canvas to get details. For this recitation we will only keep votes from the 2020  presidential election and the total 2020 population counts. You can use additional columns (e.g., Governor's elections results, voting age (VAP) population counts, or the composite Dem/Rep score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13a23f-3b65-4852-a42d-3a87867f3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_precinct_data = pd.read_csv('Map_Data/precinct-data-congress-nh.csv')\n",
    "keepcolumns = ['GEOID20','District','Total_2020_Pres','Dem_2020_Pres','Rep_2020_Pres','Total_2020_Total','White_2020_Total','Hispanic_2020_Total','Black_2020_Total','Asian_2020_Total','Native_2020_Total','Pacific_2020_Total']\n",
    "nh_precinct_data = nh_precinct_data[keepcolumns]\n",
    "nh_precinct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e75d5-6d29-4ab2-ab58-39c67f8b79f1",
   "metadata": {},
   "source": [
    "### This is the precinct boundary data (uses shapely)\n",
    "\n",
    "This is data that represents the geography of the districts. It is needed to test for contiguity, or for any districting partitioning method based on geography. The data is in Shapely format. Each district is represented as a set of points that are connected to create the district shape (in the long/lat coordinates). Shapely geometric functions can be used to compare the shapes. These can be quite inefficient to run, so I am also providing you a pre-computed index that, for each district, lists the districts that are contiguous to it. You can see the code to generate the index in Contiguity.ipynb.\n",
    "\n",
    "To manipulate the shapes, cast them into Shapely Polygons (see example below) and you can use the Polygon properties and functions: https://shapely.readthedocs.io/en/stable/reference/shapely.Polygon.html#shapely.Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3dbde7-e314-4fe0-9861-969d96e13645",
   "metadata": {},
   "outputs": [],
   "source": [
    "shpfile = 'Map_Data/nh_vtd_2020_bound/nh_vtd_2020_bound.shp'\n",
    "dbffile = 'Map_Data/nh_vtd_2020_bound/nh_vtd_2020_bound.dbf'\n",
    "shxfile = 'Map_Data/nh_vtd_2020_bound/nh_vtd_2020_bound.shx'\n",
    "\n",
    "\n",
    "shpfile = shp.Reader(shp=shpfile, shx=shxfile, dbf=dbffile)\n",
    "nh_precinct_boundaries={}\n",
    "for sr in shpfile.iterShapeRecords():\n",
    "    geom = sr.shape # get geo bit\n",
    "    rec = sr.record # get db fields\n",
    "    nh_precinct_boundaries[rec[3]]=geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba2224-64aa-485f-a3c5-1fef0feb2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shxfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac244b-1f70-461f-b40e-9123f092fb03",
   "metadata": {},
   "source": [
    "### Example: Town of Barrington precinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fc071-31a3-4ec0-8da0-540a3ee3f504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#See for instance the precinct boundaries for the town of Barrington precinct\n",
    "print(Polygon(shape(nh_precinct_boundaries['33003SAND01'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453193c1-b51a-4f24-b7a1-5ecbfd9c9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the coordinates of the bouding box of the district\n",
    "print(Polygon(shape(nh_precinct_boundaries['33017BARR01'])).bounds)\n",
    "# Or we can calculate the geographical centroid of the district\n",
    "print(Polygon(shape(nh_precinct_boundaries['33017BARR01'])).centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419e97d-3996-40e8-a93f-2722fcc710ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the data from that district from our data table\n",
    "nh_precinct_data[nh_precinct_data['GEOID20']=='33017BARR01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf870e0-f364-4ce2-9c04-0e699f7052bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the current assignment of the district\n",
    "nh_current_assignment[nh_current_assignment['GEOID20']=='33017BARR01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9050c5e-5d6a-4380-8692-326cc80e3dab",
   "metadata": {},
   "source": [
    "# How to get a district by name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd58b7-2393-4db5-96ef-a2015c8149ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The name information is in the dbf file. You need the dbfread library to convert it to a dataframe\n",
    "from dbfread import DBF\n",
    "\n",
    "dbf = DBF(dbffile)\n",
    "nh_names_df = pd.DataFrame(iter(DBF(dbffile)))\n",
    "nh_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b7e90-957f-40ba-abe8-f718d6fa5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_names_df[nh_names_df['NAME20'].str.contains('WILTON')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696388-313f-4090-9035-09a7da258a19",
   "metadata": {},
   "source": [
    "# Some possible redistricting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ba925-cd36-43d6-b2b8-76b9140c8292",
   "metadata": {},
   "source": [
    "### 1. Geographical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ced817-48c2-4b56-8a46-c274e5b9ff7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's start by creating simple geopgraphical maps, splitting the district in half North/South, or East/West.\n",
    "New Hampshire's bounding box is (-72.557247,42.69699,-70.610621,45.305476) (https://anthonylouisdagostino.com/bounding-boxes-for-all-us-states/)\n",
    "So let's start by splitting the state though the middle: everything west of longitude -71.583934 is in District 1, everything east is in District 2. We will use the precinct centroids to assign them.\n",
    "Import the Map to DRA to look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30c782-b466-47d7-bb37-78b3cad6bd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nh_longitude_assignment = nh_current_assignment.copy()\n",
    "nh_longitude_assignment['District'] = 0\n",
    "for index, row in nh_longitude_assignment.iterrows():\n",
    "    try:\n",
    "        if shape(nh_precinct_boundaries[row['GEOID20']]).type == 'Polygon':\n",
    "            centroid = Polygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        elif shape(nh_precinct_boundaries[row['GEOID20']]).type == 'MultiPolygon':\n",
    "            centroid = MultiPolygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        else:\n",
    "            print(shape(nh_precinct_boundaries[row['GEOID20']]).type)\n",
    "            pass\n",
    "        if centroid.x <= -71.583934:\n",
    "            nh_longitude_assignment.iloc[index,nh_longitude_assignment.columns.get_loc('District')] = 1\n",
    "        else:\n",
    "            nh_longitude_assignment.iloc[index,nh_longitude_assignment.columns.get_loc('District')] = 2\n",
    "    except KeyError: \n",
    "        pass\n",
    "#print(nh_longitude_assignment)\n",
    "nh_longitude_assignment.to_csv('Recitation maps/nh_centerlongitude_map.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f3c0e-8ff3-4c75-a3d3-35ab5631d45b",
   "metadata": {},
   "source": [
    "Let's now split the state though the middle horizontally: everything north of latitude 44.001233 is in District 1, everything south is in District 2. We will use the precinct centroids to assign them. \n",
    "Import the Map to DRA to look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddace2-10d0-4e2f-bd9d-8e99fee4657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_latitude_assignment = nh_current_assignment.copy()\n",
    "nh_latitude_assignment['District'] = 0\n",
    "for index, row in nh_latitude_assignment.iterrows():\n",
    "    try:\n",
    "        if shape(nh_precinct_boundaries[row['GEOID20']]).type == 'Polygon':\n",
    "            centroid = Polygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        elif shape(nh_precinct_boundaries[row['GEOID20']]).type == 'MultiPolygon':\n",
    "            centroid = MultiPolygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        else:\n",
    "            print(shape(nh_precinct_boundaries[row['GEOID20']]).type)\n",
    "            pass\n",
    "        if centroid.y <= 44.001233:\n",
    "            nh_latitude_assignment.iloc[index,nh_latitude_assignment.columns.get_loc('District')] = 1\n",
    "        else:\n",
    "            nh_latitude_assignment.iloc[index,nh_latitude_assignment.columns.get_loc('District')] = 2\n",
    "    except KeyError: \n",
    "        pass\n",
    "#print(nh_longitude_assignment)\n",
    "nh_latitude_assignment.to_csv('Recitation maps/nh_centerlatitude_map.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c92a31-4b48-40fd-8027-a4c9e1887923",
   "metadata": {},
   "source": [
    "### Look at the statistics for the maps\n",
    "Are they good maps?\n",
    "The population are completely different. We can redraw the maps using different split longitude (resp. latitude) values. You can code a binary search that stops when the two populations are within 5% for example.\n",
    "\n",
    "(There are four district for which the shapely data is not included, unfortunately this is from the census data so we will have to fix it manually later. Two of these have no population so are easy to ignore and just assign to the neighboring district. Two:  33013CONC05 and 33017SOME03 are populated district. You can hardcode their assignment for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fb803-8360-4e6e-a87b-34d1d44c9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the population of a District:\n",
    "D1_list = nh_latitude_assignment[nh_latitude_assignment['District']==1]['GEOID20']\n",
    "D2_list = nh_latitude_assignment[nh_latitude_assignment['District']==2]['GEOID20']\n",
    "popD1= nh_precinct_data[nh_precinct_data['GEOID20'].isin(D1_list)]['Total_2020_Total'].sum()\n",
    "popD2= nh_precinct_data[nh_precinct_data['GEOID20'].isin(D2_list)]['Total_2020_Total'].sum()\n",
    "print('D1 pop = '+str(popD1)+', D2 pop = '+str(popD2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554f0a0-a61f-4bf3-92f9-88143f51aa70",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's write a function that gives us the population of the districts (returns a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451608c7-8272-4ad0-8460-992c6161ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistrictPopulations(assignment,data_file, num_district):\n",
    "    population = {}\n",
    "    for i in range (1,num_district+1):\n",
    "        population[i] = data_file[data_file['GEOID20'].isin(assignment[assignment['District']==i]['GEOID20'])]['Total_2020_Total'].sum()\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b859dca-f174-40e6-af59-8466ecf2ab37",
   "metadata": {},
   "source": [
    "Let's see the split at Latitude = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff692b3-699a-4706-a1d8-eea75156b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_latitude43_assignment = nh_current_assignment.copy()\n",
    "nh_latitude43_assignment['District'] = 0\n",
    "for index, row in nh_latitude43_assignment.iterrows():\n",
    "    try:\n",
    "        if shape(nh_precinct_boundaries[row['GEOID20']]).type == 'Polygon':\n",
    "            centroid = Polygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        elif shape(nh_precinct_boundaries[row['GEOID20']]).type == 'MultiPolygon':\n",
    "            centroid = MultiPolygon(shape(nh_precinct_boundaries[row['GEOID20']])).centroid\n",
    "        else:\n",
    "            print(shape(nh_precinct_boundaries[row['GEOID20']]).type)\n",
    "            pass\n",
    "        if centroid.y <= 43:\n",
    "            nh_latitude43_assignment.iloc[index,nh_latitude43_assignment.columns.get_loc('District')] = 1\n",
    "        else:\n",
    "            nh_latitude43_assignment.iloc[index,nh_latitude_assignment.columns.get_loc('District')] = 2\n",
    "    except KeyError: \n",
    "        pass\n",
    "nh_latitude43_assignment.to_csv('Recitation maps/nh_latitude43_map.csv',index=False)\n",
    "getDistrictPopulations(nh_latitude43_assignment,nh_precinct_data,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40742510-75f1-4d29-acc9-ddb080db31d0",
   "metadata": {},
   "source": [
    "Import the map to DRA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7545e8a-b73c-4263-bac2-b5f09d7603ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Graph-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769ce22-4a65-4f41-971a-c1be277cc913",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see the map as a graph, each precinct is a vertex, with edges drawn between neighboring precincts. We will use the NetworkX graph library. This will allow us to check for connectivity (contiguity) of the districts.\n",
    "\n",
    "First, we create a district graph. For this, we use the contiguity index I have pre-computed using Contiguity.ipynb, that is stored in Contiguity_nh.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c7ed1-b569-4876-aa65-a2b707f150cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_contiguity = pd.read_csv('Contiguity_nh.csv', header=None)\n",
    "nh_contiguity.columns = ['Precinct','Neighbors']\n",
    "admin_water_precincts =['33017SOME03','33015ZZZZZZ']\n",
    "#Two precinct are administrative, have no population, and while they should be included in the assignment, they cannot be used for contiguity\n",
    "\n",
    "# Let's graph the district from the nh_latitude43_assignment map\n",
    "district1_graph = nx.Graph() #creates an empty undirected graph\n",
    "district1_nodes = nh_latitude43_assignment[nh_latitude43_assignment['District']==1]['GEOID20']\n",
    "district1_graph.add_nodes_from(district1_nodes)\n",
    "for id in district1_nodes:\n",
    "    neighbors = ast.literal_eval(nh_contiguity[nh_contiguity['Precinct']==id]['Neighbors'].values.tolist()[0])\n",
    "    # needed to convert string to list because the csv encodes the list as a string\n",
    "    for neighbor in neighbors:\n",
    "        district1_graph.add_edge(id,neighbor)\n",
    "\n",
    "print(nx.is_connected(district1_graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd178842-367b-4b03-a50e-7382ee38e0ce",
   "metadata": {},
   "source": [
    "Now let's add a disconnected precinct to the district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e571009-b5ec-49b4-9c07-dd0bddfe4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "district1_graph.add_node(\"33007JEFF01\")\n",
    "neighbors = nh_contiguity[nh_contiguity['Precinct']==\"33007JEFF01\"]['Neighbors'].values.tolist()[0].replace('[','').replace(']','').replace(' ','').split(',')\n",
    "for neighbor in neighbors:\n",
    "        district1_graph.add_edge(id,neighbor)\n",
    "print(nx.is_connected(district1_graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddbf15-a9a6-4947-a26d-090b95fccf93",
   "metadata": {},
   "source": [
    "So we can use the nx.is_connected function to check for contiguity. Here is a general function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6156-7605-472b-b3c6-f0c5f4979fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isDistrictContiguous(district_num, assignment, contiguity_list, ignore_list=[]):\n",
    "    ## input:\n",
    "    ## district_num: the district number\n",
    "    ## assignment: the assignment from precinct to district\n",
    "    ## contiguity_list: the list of neighbors for each precinct, from the csv file\n",
    "    contiguity_list.columns = ['Precinct','Neighbors']\n",
    "    district_graph = nx.Graph() #creates an empty undirected graph\n",
    "    district_nodes = assignment[assignment['District']==district_num]['GEOID20'].tolist()\n",
    "    for i in ignore_list:\n",
    "        try:\n",
    "            district_nodes.remove(i)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    district_graph.add_nodes_from(district_nodes)\n",
    "    for id in district_nodes:\n",
    "        neighbors = ast.literal_eval(contiguity_list[contiguity_list['Precinct']==id]['Neighbors'].values.tolist()[0])\n",
    "        # needed to convert string to list because the csv encodes the list as a string\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in district_nodes:\n",
    "                district_graph.add_edge(id,neighbor)\n",
    "    return nx.is_connected(district_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9106a-81ec-44a6-895a-3a574f723ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "isDistrictContiguous(2,nh_latitude43_assignment,nh_contiguity,admin_water_precincts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86799775-e0de-40fb-8d99-31545e8307a9",
   "metadata": {},
   "source": [
    "#### Flood Fill Algorithm\n",
    "Here we will use a simple flood fill algorithm which will assign half of the precinct to D2 based on a BFS traversal.\n",
    "It does not consider population, but does check that both districts stay contiguous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c90b98-0aa6-4ae5-84d0-8eabd5463c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_floodfill_assignment = nh_current_assignment.copy()\n",
    "nh_floodfill_assignment['District'] = 2 # Let's assign everything to D2, and build D1 via flood-fill.\n",
    "\n",
    "d1_size = 0\n",
    "\n",
    "next_precincts = [\"33005HINS01\"] #this is the district on the bottom left of the map. You can start anywhere else.\n",
    "visited = []\n",
    "\n",
    "while d1_size<163: #(there are 326 precincts total. We want to assign half to D1)\n",
    "    nh_floodfill_assignment.loc[nh_floodfill_assignment['GEOID20']==next_precincts[0],'District']=1\n",
    "    neighbors = ast.literal_eval(nh_contiguity[nh_contiguity['Precinct']==next_precincts[0]]['Neighbors'].values.tolist()[0])\n",
    "    for neighbor in neighbors:\n",
    "        #print(nh_floodfill_assignment[nh_floodfill_assignment['GEOID20']==neighbor]['District'].values[0])\n",
    "        #if nh_floodfill_assignment[nh_floodfill_assignment['GEOID20']==neighbor]['District'].values[0]!=1: #only add precinct we haven't visited\n",
    "        if neighbor not in visited and neighbor not in next_precincts : #only add if not visited and not already in list\n",
    "            next_precincts.append(neighbor)\n",
    "    d1_size += 1\n",
    "    visited.append(next_precincts[0])\n",
    "    next_precincts.pop(0)\n",
    "    \n",
    "    \n",
    "\n",
    "print(isDistrictContiguous(1,nh_floodfill_assignment,nh_contiguity))\n",
    "print(isDistrictContiguous(2,nh_floodfill_assignment,nh_contiguity))\n",
    "# Note: in some cases flood fill breaks contiguity, it may be a good idea to check for contiguity during flood fill.\n",
    "\n",
    "nh_floodfill_assignment.to_csv('Recitation maps/nh_floodfill_map.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecf288-4593-4072-b584-7ace5735b598",
   "metadata": {},
   "source": [
    "Import the map in DRA. It is based on balancing the number of precints, so the population counts are off. You can adapt the code to take population count into account. You can also start from another district.\n",
    "\n",
    "Note that the algorithm can break contiguity (by splitting another district), so this should be checked during the traversal. In addition, it has to be applied carefully when you have more than 2 districts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731fc9f-70de-4a63-aed2-c40a70ff439d",
   "metadata": {},
   "source": [
    "### 3. From an Existing map - Flip Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3156410-de0b-470a-8953-d1921e743d5f",
   "metadata": {},
   "source": [
    "Let's try a random-based strategy which takes a precinct on the border of the two districts and flip it to the other district.\n",
    "Let's do this 10 times starting from the current official assignment map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd10f0-5938-406c-813d-96cfb1c27af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_flipstep_assignment = nh_current_assignment.copy()\n",
    "\n",
    "## Let's do a simplistic approach. \n",
    "## First we randomly select 5 D1 precincts that border D2 and assign them to D2\n",
    "\n",
    "D1_border_precincts = []\n",
    "\n",
    "D1_list = nh_latitude_assignment[nh_latitude_assignment['District']==1]['GEOID20']\n",
    "for precinct in D1_list:\n",
    "    neighbors = ast.literal_eval(nh_contiguity[nh_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "    for neighbor in neighbors:\n",
    "        if nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==2:\n",
    "            #if one of the neighbor is in D2, then this is a border district\n",
    "            D1_border_precincts.append(precinct)\n",
    "            break\n",
    "#Sample 5 and flip them UNLESS it breaks contiguity\n",
    "flipped_precincts = np.random.choice(D1_border_precincts,5)\n",
    "for flip in flipped_precincts:\n",
    "    nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "    #check if we broke contiguity and revert the flip if we did\n",
    "    if(isDistrictContiguous(1,nh_flipstep_assignment,nh_contiguity) is False or isDistrictContiguous(2,nh_flipstep_assignment,nh_contiguity) is False):\n",
    "        nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "        print(\"Contiguity broken \" + flip)\n",
    "                                                                        \n",
    "\n",
    "## First we randomly select 5 DIFFERENT D2 precincts that border D1 and assign them to D1\n",
    "\n",
    "D2_border_precincts = []\n",
    "D2_list = nh_latitude_assignment[nh_latitude_assignment['District']==2]['GEOID20']\n",
    "for precinct in D2_list:\n",
    "    neighbors = ast.literal_eval(nh_contiguity[nh_contiguity['Precinct']==precinct]['Neighbors'].values.tolist()[0])\n",
    "    for neighbor in neighbors:\n",
    "        if nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==neighbor,'District'].values.tolist()[0]==1 and precinct not in flipped_precincts:\n",
    "            #if one of the neighbor is in D2, then this is a border district. We do not want to re-flip a district we just flipped\n",
    "            D2_border_precincts.append(precinct)\n",
    "            break\n",
    "#Sample 5 and flip them UNLESS it breaks contiguity\n",
    "flipped_precincts = np.random.choice(D2_border_precincts,5)\n",
    "for flip in flipped_precincts:\n",
    "    nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==flip,'District']=1\n",
    "    #check if we broke contiguity and revert the flip if we did\n",
    "    if(isDistrictContiguous(1,nh_flipstep_assignment,nh_contiguity) is False or isDistrictContiguous(2,nh_flipstep_assignment,nh_contiguity) is False):\n",
    "        nh_flipstep_assignment.loc[nh_flipstep_assignment['GEOID20']==flip,'District']=2\n",
    "        print(\"Contiguity broken \" + flip)\n",
    "                                                        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(isDistrictContiguous(1,nh_flipstep_assignment,nh_contiguity))\n",
    "print(isDistrictContiguous(2,nh_flipstep_assignment,nh_contiguity))\n",
    "\n",
    "nh_flipstep_assignment.to_csv('Recitation maps/nh_flipstep_assignment.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf6446-5043-4c30-abf5-7381829f7bfc",
   "metadata": {},
   "source": [
    "# Compactness\n",
    "You will also need some function to measure compactness. Here are some examples. You can write your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513250e-086e-4ece-bf7c-e01ce7a70afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_compactness(geom): # Polsby-Popper\n",
    "    p = geom.length\n",
    "    a = geom.area    \n",
    "    return (4*np.pi*a)/(p*p)\n",
    "\n",
    "def box_reock_compactness(geom): # Reock on a rectangle bounding box\n",
    "    a = geom.area \n",
    "    bb = geom.bounds # bounds gives you the minimum bounding box (rectangle)\n",
    "    bba = abs(bb[0]-bb[2])*abs(bb[1]-bb[3])\n",
    "    return a/bba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1ff00-a8bd-4b10-992a-25eb2def75b1",
   "metadata": {},
   "source": [
    "We can compute the compactness of individual precincts. Here Barrington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb897e10-e04e-4d15-9375-5687bf29362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_compactness(Polygon(shape(nh_precinct_boundaries['33003SAND01'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d786e6-2dc2-4b7b-a77f-8987c3a3f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_reock_compactness(Polygon(shape(nh_precinct_boundaries['33003SAND01'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f32691-6d9b-464d-9f4c-dc5dcc696d4f",
   "metadata": {},
   "source": [
    "To calculate the compactness of a District we need to create a shape (Multipolygon) that encloses all precincts in the District."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef08fc4-3165-4b82-b6fa-53b6ead4837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistrictShape(district_id, assignment, boundaries):\n",
    "    list_precincts = assignment[assignment['District']==district_id]['GEOID20']\n",
    "    precinct_shapes = []\n",
    "    for i in list_precincts:\n",
    "        if shape(boundaries[i]).type == 'Polygon':\n",
    "            precinct_shapes.append(Polygon(shape(boundaries[i])))\n",
    "        elif shape(boundaries[i]).type == 'MultiPolygon':\n",
    "            precinct_shapes.append(MultiPolygon(shape(boundaries[i])))      \n",
    "    district_shape = shapely.ops.unary_union(precinct_shapes)\n",
    "    #print(district_shape)\n",
    "    return district_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd71738-063f-46d3-8487-66fcce258bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compactness of the current assignment\n",
    "print(\"D1 PP : \"+str(pp_compactness(getDistrictShape(1,nh_current_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 PP : \"+str(pp_compactness(getDistrictShape(2,nh_current_assignment,nh_precinct_boundaries))))\n",
    "print(\"D1 BR : \"+str(box_reock_compactness(getDistrictShape(1,nh_current_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 BR : \"+str(box_reock_compactness(getDistrictShape(2,nh_current_assignment,nh_precinct_boundaries))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8f777-fc93-46c2-9eac-c1bee80aa35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compactness of the floodfill assignment\n",
    "print(\"D1 PP : \"+str(pp_compactness(getDistrictShape(1,nh_floodfill_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 PP : \"+str(pp_compactness(getDistrictShape(2,nh_floodfill_assignment,nh_precinct_boundaries))))\n",
    "print(\"D1 BR : \"+str(box_reock_compactness(getDistrictShape(1,nh_floodfill_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 BR : \"+str(box_reock_compactness(getDistrictShape(2,nh_floodfill_assignment,nh_precinct_boundaries))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a95c4-14cb-4b68-8354-f8ae9470c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compactness of the 43 Latitude assignment\n",
    "print(\"D1 PP : \"+str(pp_compactness(getDistrictShape(1,nh_latitude43_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 PP : \"+str(pp_compactness(getDistrictShape(2,nh_latitude43_assignment,nh_precinct_boundaries))))\n",
    "print(\"D1 BR : \"+str(box_reock_compactness(getDistrictShape(1,nh_latitude43_assignment,nh_precinct_boundaries))))\n",
    "print(\"D2 BR : \"+str(box_reock_compactness(getDistrictShape(2,nh_latitude43_assignment,nh_precinct_boundaries))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e762428-627e-431b-9178-e25c0c50f340",
   "metadata": {},
   "source": [
    "### This should give you the information you need to start creating your own maps.\n",
    "NJ_Redistricing_StartingPoint.ipnyb has an example map for NJ. \n",
    "I suggest you test your algorithms on NH first as running times will be slower on a small map. You can of course change the number of districts in your test code you test different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deec74-ffb2-439f-8c40-20ec7f42ac3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
